# -*- coding: utf-8 -*-
"""WGISD SFT tests

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Vq_h7Wj76pGyuKoeePinuykTyyAlKqtH

Install detectron2 dependencies
"""

"""Based on code by A. Ceruti, extended to support Surgical Fine Tuning

Classify as grape bunch or not (binary)
"""

##Register data

from detectron2.data.datasets import register_coco_instances
import os
from detectron2.data.datasets import load_coco_json
from detectron2.data import MetadataCatalog


metadata_dict_training = {'name' : 'wgisd_train', 'thing_classes' : ['grape bunch'], 'thing_colors' : [(255, 0, 0)]}
metadata_dict_test = {'name' : 'wgisd_test', 'thing_classes' : ['grape bunch'], 'thing_colors' : [(255, 0, 0)]}

register_coco_instances("wgisd_train", metadata_dict_training, "../data/wgisd_original_split/training/annotations.json", "../data/wgisd_original_split/training/images")
register_coco_instances("wgisd_test", metadata_dict_test, "../data/wgisd_original_split/test/annotations.json", "../data/wgisd_original_split/test/images")


annotations_train = os.path.join("../data/wgisd_original_split/", "training/annotations.json") 
annotations_test = os.path.join("../data/wgisd_original_split/", "test/annotations.json") 

training_dict = load_coco_json(annotations_train, "../data/wgisd_original_split/training/images", dataset_name="wgisd_train")
training_metadata = MetadataCatalog.get("wgisd_train")
test_dict = load_coco_json(annotations_test, "../data/wgisd_original_split/test/images", dataset_name="wgisd_test")
test_metadata = MetadataCatalog.get("wgisd_test")

"""Data augmentation"""

import detectron2.data.transforms as T

def create_augmentations_train(cfg):
    augmentations = []    
    _PROB_HIGH = 0.5
    _PROB_LOW = 0.33

    #augmentations.append(T.ResizeShortestEdge(cfg.INPUT.MIN_SIZE_TRAIN, cfg.INPUT.MAX_SIZE_TRAIN, cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING, interp=Image.NEAREST))

    augmentations.append(T.RandomApply(T.RandomBrightness(0.75, 1.25), prob=_PROB_LOW)) 
    augmentations.append(T.RandomApply(T.RandomContrast(0.75, 1.25), prob=_PROB_LOW))
    augmentations.append(T.RandomApply(T.RandomSaturation(0.75, 1.25), prob=_PROB_LOW))
    #augmentations.append(T.Noise()) #probability inside the class, default is Gaussian   #not in detectron2 latest
    #augmentations.append(T.Blur()) #probability inside the class, default is Gaussian    #not in detectron2 latest
    #augmentations.append(T.PixelDropout(prob=0.33))                                      #not in detectron2 latest

    augmentations.append(T.RandomFlip(prob = _PROB_HIGH, horizontal=True, vertical=False))
    return augmentations


def create_augmentations_test(cfg):
    augmentations = []
    return augmentations

"""WGISD category labels are divided by grape species, instead we want to load binary annotations (prepared by Ceruti, under /wgisd_original_split/)"""



import detectron2.data.detection_utils as utils
from detectron2.data import DatasetMapper, detection_utils
import copy

class Mapper(DatasetMapper): 

    def __call__(self, dataset_dict):
        """
        Args:
            dataset_dict (dict): Metadata of one image, in Detectron2 Dataset format.

        Returns:
            dict: a format that builtin models in detectron2 accept
        """
        dataset_dict = copy.deepcopy(dataset_dict)  # it will be modified by code below
        image = utils.read_image(dataset_dict["file_name"], format="BGR") #so I can apply saturation
        detection_utils.check_image_size(dataset_dict, image)
        image = image[:,:,::-1]
        aug_input = T.AugInput(image)
        transforms = self.augmentations(aug_input)
        image, sem_seg_gt = aug_input.image, aug_input.sem_seg

        image_shape = image.shape[:2]  # h, w
        # Pytorch's dataloader is efficient on torch.Tensor due to shared-memory,
        # but not efficient on large generic data structures due to the use of pickle & mp.Queue.
        # Therefore it's important to use torch.Tensor.
        image = image[:,:,::-1]
        dataset_dict["image"] = torch.as_tensor(np.ascontiguousarray(image.transpose(2,0,1)))
        if sem_seg_gt is not None:
            dataset_dict["sem_seg"] = torch.as_tensor(sem_seg_gt.astype("long"))

        if not self.is_train:
            # USER: Modify this if you want to keep them for some reason.
            dataset_dict.pop("annotations", None)
            dataset_dict.pop("sem_seg_file_name", None)
            return dataset_dict

        if "annotations" in dataset_dict:
            self._transform_annotations(dataset_dict, transforms, image_shape)

        return dataset_dict

"""LossEvalHook and EarlyStopping are not in the original detectron2 package -- declared in the following as custom classes"""

import numpy as np
from detectron2.data import build_detection_train_loader
from detectron2.engine.hooks import HookBase
from detectron2.evaluation import inference_context
from detectron2.utils.logger import log_every_n_seconds
from detectron2.data import build_detection_test_loader
import detectron2.utils.comm as comm
import torch
import time
import datetime
import logging

class LossEvalHook(HookBase):
    def __init__(self, cfg, model, data_loader):
        self._model = model
        self._period = cfg.TEST.EVAL_PERIOD
        self._root = cfg.OUTPUT_DIR
        self._data_loader = data_loader
        self._min_mean_loss = 0.0
        self._bfirst = True
    
    def _do_loss_eval(self):
        # Copying inference_on_dataset from evaluator.py
        total = len(self._data_loader)
        num_warmup = min(5, total - 1)
            
        start_time = time.perf_counter()
        total_compute_time = 0
        losses = []
        for idx, inputs in enumerate(self._data_loader):            
            if idx == num_warmup:
                start_time = time.perf_counter()
                total_compute_time = 0
            start_compute_time = time.perf_counter()
            if torch.cuda.is_available():
                torch.cuda.synchronize()
            total_compute_time += time.perf_counter() - start_compute_time
            iters_after_start = idx + 1 - num_warmup * int(idx >= num_warmup)
            seconds_per_img = total_compute_time / iters_after_start
            if idx >= num_warmup * 2 or seconds_per_img > 5:
                total_seconds_per_img = (time.perf_counter() - start_time) / iters_after_start
                eta = datetime.timedelta(seconds=int(total_seconds_per_img * (total - idx - 1)))
                log_every_n_seconds(
                    logging.INFO,
                    "Loss on Validation  done {}/{}. {:.4f} s / img. ETA={}".format(
                        idx + 1, total, seconds_per_img, str(eta)
                    ),
                    n=5,
                )
            loss_batch = self._get_loss(inputs)
            losses.append(loss_batch)
        mean_loss = np.mean(losses)
        self.trainer.storage.put_scalar('validation_loss', mean_loss, smoothing_hint = False)
        comm.synchronize()
        return mean_loss
            
    def _get_loss(self, data):
        # How loss is calculated on train_loop 
        metrics_dict = self._model(data)
        metrics_dict = {
            k: v.detach().cpu().item() if isinstance(v, torch.Tensor) else float(v)
            for k, v in metrics_dict.items()
        }
        total_losses_reduced = sum(loss for loss in metrics_dict.values())
        return total_losses_reduced
                
    def after_step(self):
        next_iter = self.trainer.iter + 1
        is_final = next_iter == self.trainer.max_iter
        if is_final or (self._period > 0 and next_iter % self._period == 0):
            mean_loss = self._do_loss_eval()
            if self._bfirst:
                self._min_mean_loss = mean_loss
                self._bfirst = False
            #-------- save best model according to metrics --------
            if mean_loss < self._min_mean_loss:
                self._min_mean_loss = mean_loss
                self.trainer.checkpointer.save('model_best_validationLoss')
                with open(os.path.join(self._root, 'bestiter.txt'), 'a+') as f: 
                    f.write('min val loss: ' + str(mean_loss) + ' at iter: ' + str(self.trainer.iter) + '\n')

class EarlyStopping(HookBase):
    def __init__(self, cfg, patience):
        self._patience = patience
        self._period = cfg.TEST.EVAL_PERIOD
        self._count = 0 #count for patience
        self._bfirst = True
        self._current_best_iteration = 0
        self._current_best_validation_loss = 0
        self._metric = "validation_loss"
    
    def after_step(self):
        
        next_iter = self.trainer.iter + 1
        is_final = next_iter == self.trainer.max_iter        

        if is_final or (self._period > 0 and next_iter % self._period == 0): #this should be called at each validation period after the loss has been already stored
            
            #now read the best loss and the current loss
            metric_tuple = self.trainer.storage.latest().get(self._metric)
            latest_metric, metric_iter = metric_tuple

            if self._bfirst:              
                self._bfirst = False
                self._current_best_iteration, self._current_best_validation_loss = metric_iter, latest_metric

            else:                               
                if(latest_metric > self._current_best_validation_loss):
                    self._count = self._count + 1
                    if (self._count == self._patience): # ho raggiunto la patience e devo terminare il training loop
                        raise ValueError("Early stopping at iteration %d, because patience of %d reached"%(self.trainer.iter, self._count))
                else:
                    self._current_best_iteration, self._current_best_validation_loss = metric_iter, latest_metric
                    self._count = 0

            log_every_n_seconds(
                    logging.INFO,
                    "EARLYSTOPPING: current val_loss {} at iter {} || best val loss {} at iter {}|| patience: {}".format(
                        latest_metric, metric_iter, self._current_best_validation_loss, self._current_best_iteration, self._count
                    ),
                    n=2,
                )

"""Detectron2 default trainer"""

from detectron2.engine import hooks
from fvcore.nn.precise_bn import get_bn_modules
from detectron2.data import build_detection_test_loader, build_detection_train_loader
import detectron2.utils.comm as comm
from detectron2.engine import DefaultTrainer
from detectron2.evaluation import COCOEvaluator

class Trainer(DefaultTrainer):

    def build_hooks(self):
      cfg = self.cfg.clone()
      cfg.defrost()
      cfg.DATALOADER.NUM_WORKERS = 0  # save some memory and time for PreciseBN

      ret = [
          hooks.IterationTimer(),
          hooks.LRScheduler(),
          hooks.PreciseBN(
              # Run at the same freq as (but before) evaluation.
              cfg.TEST.EVAL_PERIOD,
              self.model,
              # Build a new data loader to not affect training
              self.build_train_loader(cfg),
              cfg.TEST.PRECISE_BN.NUM_ITER,
          )
          if cfg.TEST.PRECISE_BN.ENABLED and get_bn_modules(self.model)
          else None,
      ]

      # Do PreciseBN before checkpointer, because it updates the model and need to
      # be saved by checkpointer.
      # This is not always the best: if checkpointing has a different frequency,
      # some checkpoints may have more precise statistics than others.
      if comm.is_main_process():
          ret.append(hooks.PeriodicCheckpointer(self.checkpointer, cfg.SOLVER.CHECKPOINT_PERIOD))

      def test_and_save_results():
          self._last_eval_results = self.test(self.cfg, self.model)
          return self._last_eval_results

      # Do evaluation after checkpointer, because then if it fails,
      # we can use the saved checkpoint to debug.
      #ret.append(hooks.EvalHook(cfg.TEST.EVAL_PERIOD, test_and_save_results))
      #ret.append(hooks.BestCheckpointer(cfg.TEST.EVAL_PERIOD, self.checkpointer, "bbox/AP50", "max", file_prefix="model_best_bbox"))
      #ret.append(hooks.BestCheckpointer(cfg.TEST.EVAL_PERIOD, self.checkpointer, "segm/AP50", "max", file_prefix="model_best_segm"))
      ##TODO HERE: add the BestCKPT hook on validation loss:
      #1-Bestckpt will save the model_best 
      #2-create Early stopping hook that: reads from metrics.json the best val loss iteration and run the logic
      #ret.append(hooks.BestCheckpointer(cfg.TEST.EVAL_PERIOD, self.checkpointer, "validation_loss", "max", file_prefix="model_best_validationLoss"))

      #mapper = Mapper(cfg, is_train=True, augmentations=[])
      #ret.append(LossEvalHook(self.cfg, self.model, build_detection_test_loader(self.cfg, self.cfg.DATASETS.TEST[0], mapper)))
      #ret.append(hooks.BestCheckpointer(cfg.TEST.EVAL_PERIOD, self.checkpointer, "validation_loss", "min", file_prefix="model_valLoss"))

      if comm.is_main_process():
          # Here the default print/log frequency of each writer is used.
          # run writers in the end, so that evaluation metrics are written
          ret.append(hooks.PeriodicWriter(self.build_writers(), period=39))

      #append our EarlyStopping Hook in order to stop computation if the patience is reached (patience is 25 * validation period )
      #ret.append(EarlyStopping(self.cfg, 30))

      return ret


    @classmethod
    def build_train_loader(cls, cfg):       
        custom_mapper = Mapper(cfg, is_train=True, augmentations=create_augmentations_train(cfg))
        return build_detection_train_loader(cfg, mapper=custom_mapper)

    @classmethod
    def build_test_loader(cls, cfg, dataset_name):       
        custom_mapper = Mapper(cfg, is_train=True, augmentations=[])
        return build_detection_test_loader(cfg, dataset_name, mapper = custom_mapper)


    @classmethod
    def build_evaluator(cls, cfg, dataset_name, output_folder=None):
        if output_folder is None:
            output_folder = os.path.join(cfg.OUTPUT_DIR,"detectron2_wgisd_eval") 
        return COCOEvaluator(dataset_name, ("bbox", "segm"), True, output_folder, use_fast_impl=False)

"""Load custom config model with SFT_AT parameter"""

from detectron2.config import get_cfg
from detectron2 import model_zoo

cfg = get_cfg()
custom_cfg ='Misc/scratch_mask_rcnn_R_50_FPN_9x_gn_SFT.yaml' #path must start relative to detectron2/configs location
cfg.merge_from_file(model_zoo.get_config_file(custom_cfg))



cfg.DATALOADER.NUM_WORKERS = 2
cfg.DATASETS.TRAIN = ("wgisd_train",)

cfg.INPUT.MIN_SIZE_TRAIN = (1365)
cfg.INPUT.MIN_SIZE_TEST = (1365)
cfg.INPUT.MAX_SIZE_TRAIN = 2048
cfg.INPUT.MAX_SIZE_TEST = 2048
cfg.INPUT.MIN_SIZE_TRAIN_SAMPLING = "choice"
cfg.INPUT.FORMAT = "BGR"

#cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(custom_cfg) #they are "" in the config
cfg.MODEL.BACKBONE.FREEZE_AT = 0
cfg.MODEL.BACKBONE.SFT_AT = 2 #must be <= 5 (5 resnet blocks, where 1 is the stem)
cfg.MODEL.ROI_HEADS.FREEZE = True # if True freeze ROI heads
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1
cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.9 
cfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.5
cfg.MODEL.FPN.FREEZE = True  # if True, applies SFT to same res block as SFT-ed backbone. Set False if only stem is SFT-ed
cfg.MODEL.FPN.JOINT_SFT= True
cfg.MODEL.RPN.FREEZE = True # If True, freeze RPN heads

#SOLVER OPTIONS
cfg.SOLVER.MAX_ITER = 2750
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.WARMUP_ITERS = 165
cfg.SOLVER.BASE_LR = 0.00125
cfg.SOLVER.STEPS = [1925, 2475] # 5% warmup, 5-70 % + 70-90% 0.0001, 
#cfg.SOLVER.MAX_ITER = 10000 

cfg.TEST.EVAL_PERIOD = 39  
cfg.TEST.AUG.MIN_SIZES = (1365)
cfg.TEST.AUG.MAX_SIZE = 2048

#BGR IMAGENET 
cfg.MODEL.PIXEL_MEAN =  [103.530, 116.280, 123.675]                              #BGR: [113.603, 123.196, 118.612]          RGB: [118.612, 123.196, 113.603]
cfg.MODEL.PIXEL_STD = [1.0, 1.0, 1.0]                                            #BGR: [0.30522742, 0.26201303, 0.25535114]    RGB: [0.25535114, 0.26201303, 0.30522742]

cfg.OUTPUT_DIR = "./RGB_MaskRCNN_SFT_wgisd_output"


print("################################ CFG personalized")
print(cfg)

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)
trainer = Trainer(cfg)
trainer.resume_or_load(resume=False)

"""Check if parameters will be frozen or not"""

from detectron2.modeling import build_model
model = build_model(cfg)

cnt =0
for name, param in model.named_parameters():
    if param.requires_grad:
      print(name)
      cnt+=1
print("%i parameters to be trained" % cnt)

"""Training starts here"""

"""trainer.train()

\"""Visualize losses\"""

import json
import matplotlib.pyplot as plt

experiment_folder = '/content/RGB_MaskRCNN_SFT_wgisd_output'

def load_json_arr(json_path):
    lines = []
    with open(json_path, 'r') as f:
        for line in f:
            lines.append(json.loads(line))
    return lines

experiment_metrics = load_json_arr(experiment_folder + '/metrics.json')
iters_total_loss = [x['iteration'] for x in experiment_metrics if 'total_loss' in x]
total_loss = [x['total_loss'] for x in experiment_metrics if 'total_loss' in x]
iters_val_loss = [x['iteration'] for x in experiment_metrics if 'validation_loss' in x]
val_loss = [x['validation_loss'] for x in experiment_metrics if 'validation_loss' in x]
iters_ap50 = [x['iteration'] for x in experiment_metrics if 'bbox/AP50' in x]
ap50 = [x['bbox/AP50'] for x in experiment_metrics if 'bbox/AP50' in x]

fig, ax = plt.subplots()
ax.plot(iters_total_loss, total_loss)
ax.plot(iters_val_loss, val_loss)
ax.set_xlabel('iteration')
ax.set_ylabel('loss')
ax.legend(['total_loss', 'validation_loss'], loc='best')
iter = val_loss.index(min(val_loss))
ax.vlines(iters_val_loss[iter], 0, float(max(val_loss)),color="red")
ax.annotate('min val loss: %f at iter: %d'%(float(min(val_loss)),int(iters_val_loss[iter])),xy=(iters_val_loss[iter],min(val_loss)),xytext=(+15,+15),textcoords='offset points',fontsize=8) #arrowprops=dict(arrowstyle='->',connectionstyle='arc3,rad=.2')
ax.set_xlim([0,max(max(iters_total_loss),max(iters_val_loss))])
ax.set_ylim([0,5.0])

plt.show()
path = experiment_folder + '/loss.jpg'
plt.savefig(path)
plt.close()

# Commented out IPython magic to ensure Python compatibility.
# %load_ext tensorboard
# %tensorboard --logdir output

\"""Retrieve best model to run inference on test\"""

from detectron2.engine import DefaultPredictor
from detectron2.evaluation import inference_on_dataset

cfg_test = cfg
cfg_test.MODEL.WEIGHTS = os.path.join("/RGB_MaskRCNN_SFT_wgisd_output", "model_final.pth")


predictor = DefaultPredictor(cfg_test)
evaluator = COCOEvaluator("wgisd_test", ("bbox", "segm",), False, output_dir="./output_best_wgisd/", use_fast_impl=False)
test_mapper = Mapper(cfg_test, is_train=False, augmentations=[])
test_loader = build_detection_test_loader(cfg_test, "wgisd_test", test_mapper)
print(inference_on_dataset(predictor.model, test_loader, evaluator))

\"""TODO: split WGISD by grape type\"""


"""
